from stanfordcorenlp import StanfordCoreNLP
import json
import logging
import re
import os
import numpy as np
import scipy.sparse as sp
from copy import deepcopy


"""
stanford-corenlp-full-2018-02-27 (v3.9.1)
"""

KEYS = {
    "co_ref": "co_ref",
    "adj_sent": "adj_sent",
    "self_node": "self_node",
    "adj_word": "adj_word",
}


def gen_deps(split='dev'):
    corenlp_home = '/home/jp/tools/stanford-corenlp-full-2018-02-27'
    # *.json with dependencies generated by `StanfordCoreNLP`
    out_dir = '/home/jp/workspace2/DocRED/code/prepro_data/dep'
    # dev_train.json, dev_test.json
    in_dir = '/home/jp/workspace2/DocRED/code/prepro_data'

    if not os.path.exists(out_dir):
        os.mkdir(out_dir)

    start_idx = 0
    bad_counter = 0
    props = {
        'annotators': 'tokenize,ssplit,parse,coref',
        'pipelineLanguage': 'en',
        'outputFormat': 'json',
        'tokenize.whitespace': True,  # the input is already tokenized
        'ssplit.eolonly': True,        # the input is already tokenized
        'timeout': 900000
    }
    with StanfordCoreNLP(corenlp_home, memory='6g',
                         quiet=True, logging_level=logging.WARNING, timeout=900000) as nlp:

        for ex_idx, ex in enumerate(json.load(open(f"{in_dir}/dev_{split}.json"))[start_idx:], start_idx):
            doc_key = re.sub(r'[^0-9a-zA-Z_-]', '_', ex['title'])
            dep_out_path = "{}/{}_{:>04}_{}.json".format(out_dir, split, ex_idx, doc_key)
            print(f"Start to parse {dep_out_path} ...")
            doc_len = 0
            sent_lens, sents = [], []
            for _sent in ex['sents']:
                _toks = []
                for _tok in _sent:
                    if _tok == '':  # fix
                        _tok = '-'
                        bad_counter += 1
                    _tok = re.sub(r'\s', '-', _tok)
                    _toks.append(_tok)
                    assert len(re.findall(r'\s', _tok)) == 0, f"{_sent}"
                sent_lens.append(len(_toks))
                sents.append(" ".join(_toks))
            doc_len = sum(sent_lens)
            text = "\n".join(sents)  # sentences split by \n, tokens split by whitespace
            assert len(re.split(r'\s', text)) == doc_len
            json_obj = json.loads(nlp.annotate(text, props))
            assert len(json_obj['sentences']) == len(sent_lens)
            for sent_idx, sent in enumerate(json_obj['sentences']):
                if len(sent['tokens']) != sent_lens[sent_idx]:
                    print(f"sent_idx: {sent_idx}, {sent_lens[sent_idx]} <> {len(sent['tokens'])}")
                    print(ex['sents'][sent_idx])
                    print([_t['originalText'] for _t in sent['tokens']])
                    assert False, "len(sent['tokens']) != sent_lens[sent_idx]"
            json.dump(json_obj, open(dep_out_path, 'w', encoding='utf8'), indent=2)
    print(f"token is '' : {bad_counter}")


def get_doc_length_from_ex(ex):
    return sum([len(s) for s in ex['sents']])


def get_doc_len_from_deps(deps):
    return sum([len(s['tokens']) for s in deps['sentences']])


def get_token_list(ex):
    toks = []
    for sent in ex['sents']:
        toks.extend(sent)
    return toks


def get_doc_length(ex, deps):
    l1 = get_doc_length_from_ex(ex)
    l2 = get_doc_len_from_deps(deps)
    assert l1 == l2
    return l1


def check_sparse_adj_matrix(sparse_adj_matrix):
    assert len(sparse_adj_matrix["ind"]["row"]) == len(sparse_adj_matrix["ind"]["col"])
    assert len(sparse_adj_matrix["ind"]["row"]) == len(sparse_adj_matrix["data"])


def get_self_node(doc_len):
    ret = {
        "ind": {
            "row": list(range(doc_len)),
            "col": list(range(doc_len))
        },
        "data": [1]*doc_len
    }
    check_sparse_adj_matrix(ret)
    return ret


def get_adj_word(doc_len):
    ret = {
        "ind": {
            "row": list(range(doc_len-1)),
            "col": list(range(1, doc_len))
        },
        "data": [1]*(doc_len-1)
    }
    check_sparse_adj_matrix(ret)
    return ret


def get_dependencies(deps):
    rows, cols, vals = {}, {}, {}
    adj_sent = {
        "ind": {
            "row": [],
            "col": []
        },
        "data": []
    }
    abs_start_idx = 0
    prev_sent = None
    for sent_idx, sent in enumerate(deps['sentences']):
        for dep in sent['basicDependencies']:
            dep_type = dep['dep']
            if dep_type == "ROOT":
                if sent_idx == 0:  # First sentence
                    prev_sent = dep['dependent']-1 + abs_start_idx
                else:  # the rest
                    adj_sent["ind"]["row"].append(prev_sent)
                    prev_sent = dep['dependent']-1 + abs_start_idx
                    adj_sent["ind"]["col"].append(prev_sent)
                    adj_sent["data"].append(1)
            else:
                if dep_type not in rows:
                    rows[dep_type] = []
                if dep_type not in cols:
                    cols[dep_type] = []
                if dep_type not in vals:
                    vals[dep_type] = []
                rows[dep_type].append(dep['governor']-1 + abs_start_idx)
                cols[dep_type].append(dep['dependent']-1 + abs_start_idx)
                vals[dep_type].append(1)
        abs_start_idx += len(sent['tokens'])
    assert len(adj_sent["ind"]["row"])+1 == len(deps['sentences'])
    out_dict = {
        _dep_type: {
            "ind": {
                "row": deepcopy(rows[_dep_type]),
                "col": deepcopy(cols[_dep_type])
            },
            "data": deepcopy(vals[_dep_type])
        } for _dep_type in rows.keys()
    }
    out_dict.update({KEYS["adj_sent"]: deepcopy(adj_sent)})
    for k in rows.keys():
        check_sparse_adj_matrix(out_dict[k])
    return out_dict


def get_entity_coref(ex):
    coref = {
        "ind": {
            "row": [],
            "col": []
        },
        "data": []
    }
    for entity in ex['vertexSet']:
        for m_i in range(len(entity)-1):
            for m_j in range(m_i+1, len(entity)):
                coref["ind"]['row'].append(entity[m_i]['pos'][0])
                coref["ind"]['col'].append(entity[m_j]['pos'][0])
                coref["data"].append(1)
    check_sparse_adj_matrix(coref)
    return coref


def dump_CDR_sparse_adj_matrices():
    split = "test"
    num_examples = 500
    in_dir = 'E:/workspace/repo/bran/data/cdr/CDR_DevelopmentSet.PubTator.txt/train_test_dev'
    deps_dir = "E:/workspace/repo/bran/data/cdr/CDR_DevelopmentSet.PubTator.txt/train_test_dev/dep"

    out_dir = "E:/workspace/repo/bran/data/cdr/CDR_DevelopmentSet.PubTator.txt/train_test_dev"

    adj_matrix = {}
    data = json.load(open(f"{in_dir}/dev_{split}.json"))
    for ex_idx in range(num_examples):
        ex = data[ex_idx]
        doc_key = ex['title']
        assert doc_key not in adj_matrix
        in_file_path = "{}/{}_{:>03}_{}.json".format(deps_dir, split, ex_idx, doc_key)
        deps = json.load(open(in_file_path))
        doc_len = get_doc_length(ex, deps)
        adj_matrix[doc_key] = {}
        adj_matrix[doc_key][KEYS["self_node"]] = get_self_node(doc_len)
        adj_matrix[doc_key][KEYS["adj_word"]] = get_adj_word(doc_len)
        coref_m = get_entity_coref(ex)
        if coref_m['data']:  # is not empty list
            adj_matrix[doc_key][KEYS["co_ref"]] = coref_m
        adj_matrix[doc_key].update(get_dependencies(deps))
        # token_list = get_token_list(ex)
        # for t_idx, t in enumerate(token_list):
        #     print(f"{t}-{t_idx}", end=' ')
        # for t_idx, t in enumerate(token_list):
        #     print(t, end=' ')
        # for k, v in adj_matrix[doc_key].items():
        #     _adj_mat = sp.coo_matrix((v['data'], (v['ind']['row'], v['ind']['col'])), shape=(doc_len, doc_len)).toarray()
        #     print(f"========={k}=======================================")
        #     for _r_idx, _c_idx in zip(v['ind']['row'], v['ind']['col']):
        #         print(f"({token_list[_r_idx]} ->--{k}--> {token_list[_c_idx]})")
    json.dump(adj_matrix, open(f"{out_dir}/dev_{split}_adj_matrix.json", 'w'))


def dump_DocRED_sparse_adj_matrices():
    split = "test"
    num_examples = 1000
    in_dir = '/home/jp/workspace2/DocRED/code/prepro_data'
    deps_dir = "/home/jp/workspace2/DocRED/code/prepro_data/dep"

    out_dir = "/home/jp/workspace2/DocRED/code/prepro_data"

    adj_matrix = {}
    data = json.load(open(f"{in_dir}/dev_{split}.json"))
    for ex_idx in range(num_examples):
        ex = data[ex_idx]
        assert ex['index'] == ex_idx
        doc_key = re.sub(r'[^0-9a-zA-Z_-]', '_', ex['title'])
        assert doc_key not in adj_matrix
        in_file_path = "{}/{}_{:>04}_{}.json".format(deps_dir, split, ex_idx, doc_key)
        deps = json.load(open(in_file_path))
        doc_len = get_doc_length(ex, deps)
        doc_key = ex['index']
        adj_matrix[doc_key] = {}
        adj_matrix[doc_key][KEYS["self_node"]] = get_self_node(doc_len)
        adj_matrix[doc_key][KEYS["adj_word"]] = get_adj_word(doc_len)
        coref_m = get_entity_coref(ex)
        if coref_m['data']:  # is not empty list
            adj_matrix[doc_key][KEYS["co_ref"]] = coref_m
        adj_matrix[doc_key].update(get_dependencies(deps))
        # token_list = get_token_list(ex)
        # for t_idx, t in enumerate(token_list):
        #     print(f"{t}-{t_idx}", end=' ')
        # for t_idx, t in enumerate(token_list):
        #     print(t, end=' ')
        # for k, v in adj_matrix[doc_key].items():
        #     _adj_mat = sp.coo_matrix((v['data'], (v['ind']['row'], v['ind']['col'])), shape=(doc_len, doc_len)).toarray()
        #     print(f"========={k}=======================================")
        #     for _r_idx, _c_idx in zip(v['ind']['row'], v['ind']['col']):
        #         print(f"({token_list[_r_idx]} ->--{k}--> {token_list[_c_idx]})")
    json.dump(adj_matrix, open(f"{out_dir}/dev_{split}_adj_matrix.json", 'w'))


def dump_edgeType2idx():
    with open(r'E:\workspace\repo\bran\data\cdr\CDR_DevelopmentSet.PubTator.txt\train_test_dev\edge_type.txt') as f, \
            open(r'E:\workspace\repo\bran\data\cdr\CDR_DevelopmentSet.PubTator.txt\train_test_dev\edgeType2idx.json', 'w') as outf:
        idx = 0
        out_dict = {}
        for line in f:
            et, _ = line.strip().split('\t')
            out_dict[et] = idx
            idx += 1
        json.dump(out_dict, outf, indent=2)


# dump_DocRED_sparse_adj_matrices()
# gen_deps(split='test')
dump_DocRED_sparse_adj_matrices()













